{
 "cells": [
  {
   "source": [
    "# Introduction\n",
    "\n",
    "This is a follow-up on the data preperation notebook. In this notebook you can find our experiments with the model and optimalisations.\n",
    "\n",
    "The notebook is setup as followed:\n",
    "\n",
    "1. Imports (importing necessary libraries to run the code)\n",
    "1. Functions (creating necessary functions to run the code)\n",
    "1. Final data preperations (final data preperation steps taken before training the model)\n",
    "1. Model 1 (first version of the Artificial Neural Network (ANN) model used to create the SDM)\n",
    "1. Model 2 (second version of the ANN modek used to create the SDM)\n",
    "1. Model 3 (final base model)\n",
    "1. Visualize the predictions with the final base model\n",
    "1. Visualisations with SDM optimalisations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025222,
     "end_time": "2020-11-18T12:30:30.233013",
     "exception": false,
     "start_time": "2020-11-18T12:30:30.207791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Imports\n",
    "Importing necessairy libraries to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-18T12:30:30.291879Z",
     "iopub.status.busy": "2020-11-18T12:30:30.291108Z",
     "iopub.status.idle": "2020-11-18T12:30:36.236914Z",
     "shell.execute_reply": "2020-11-18T12:30:36.237444Z"
    },
    "papermill": {
     "duration": 5.978748,
     "end_time": "2020-11-18T12:30:36.237634",
     "exception": false,
     "start_time": "2020-11-18T12:30:30.258886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from pathlib import Path\n",
    "repo_path = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025118,
     "end_time": "2020-11-18T12:30:36.288536",
     "exception": false,
     "start_time": "2020-11-18T12:30:36.263418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Functions\n",
    "Creating necessary functions to run the code. Not all functions are mentioned here for practical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-18T12:30:36.356937Z",
     "iopub.status.busy": "2020-11-18T12:30:36.355887Z",
     "iopub.status.idle": "2020-11-18T12:30:36.359165Z",
     "shell.execute_reply": "2020-11-18T12:30:36.358583Z"
    },
    "papermill": {
     "duration": 0.045997,
     "end_time": "2020-11-18T12:30:36.359339",
     "exception": false,
     "start_time": "2020-11-18T12:30:36.313342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create categorical values (from string to int)\n",
    "def create_cat_values(column, column_name):\n",
    "    \"\"\"\n",
    "    column      = the column of a dataframe that will be created categorical values of\n",
    "    column_name = the name of the column\n",
    "    \"\"\"\n",
    "    column = column.copy()\n",
    "    column = column.astype('category')\n",
    "    d = column.cat.categories\n",
    "    column = column.cat.codes\n",
    "    uniques = len(column.unique())\n",
    "    print(f'{column_name} contains {str(uniques)} categories')\n",
    "    return column, d\n",
    "\n",
    "# Plotting the loss of a model\n",
    "def plot_loss(model_name, model):\n",
    "    loss = model.history['loss']\n",
    "    plt.plot(loss)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(f'Loss of model {model_name} = {min(loss)}')\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the accuracy of a model\n",
    "def plot_accuracy(model_name, model):\n",
    "    acc = model.history['accuracy']\n",
    "    plt.plot(acc)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(f'Accuracy of model {model_name} = {max(acc)}')\n",
    "    plt.show()\n",
    "    \n",
    "# Plotting the sparce top-k categorical accuracy of a model\n",
    "def plot_sparse_accuracy(model_name, model):\n",
    "    acc = model.history['sparse_top_k_categorical_accuracy']\n",
    "    plt.plot(acc)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(f'Accuracy of model {model_name} = {max(acc)}')\n",
    "    plt.show()\n",
    "    \n",
    "# Plotting the correlation matrix of a DataFrame\n",
    "def plot_correlation_matrix(dataframe):\n",
    "    corrmatrix = dataframe.corr()\n",
    "    mask = np.zeros_like(corrmatrix)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    with sns.axes_style(\"white\"):\n",
    "        f, ax = plt.subplots(figsize=(7, 5))\n",
    "        ax = sns.heatmap(corrmatrix, mask=mask, vmax=.3, square=True)\n",
    "\n",
    "def get_index_of_observations(y_test):\n",
    "    \n",
    "    np.asarray(y_test).astype('float32').reshape((-1,5))\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    i_list_vlinder_1 = []\n",
    "    i_list_vlinder_2 = []\n",
    "    i_list_vlinder_3 = []\n",
    "    i_list_vlinder_4 = []\n",
    "    i_list_vlinder_5 = []\n",
    "\n",
    "    for items in y_test:\n",
    "        v1 = items[0]\n",
    "        v2 = items[1]\n",
    "        v3 = items[2]\n",
    "        v4 = items[3]\n",
    "        v5 = items[4]\n",
    "\n",
    "        if v1 == 1:\n",
    "            i_list_vlinder_1.append(i)\n",
    "\n",
    "        if v2 == 1:\n",
    "            i_list_vlinder_2.append(i)\n",
    "\n",
    "        if v3 == 1:\n",
    "            i_list_vlinder_3.append(i)\n",
    "\n",
    "        if v4 == 1:\n",
    "            i_list_vlinder_4.append(i)\n",
    "\n",
    "        if v5 == 1:\n",
    "            i_list_vlinder_5.append(i)\n",
    "\n",
    "        i = i + 1\n",
    "    return i_list_vlinder_1, i_list_vlinder_2, i_list_vlinder_3, i_list_vlinder_4, i_list_vlinder_5\n",
    "\n",
    "\n",
    "def threshold(threshold, y2_pred):\n",
    "    \n",
    "    y2_pred_threshold = y2_pred.copy()\n",
    "\n",
    "    for idx1, a in enumerate(y2_pred):\n",
    "        for idx2, element in enumerate(a):\n",
    "\n",
    "            if a[idx2] > threshold:\n",
    "                y2_pred_threshold[idx1, idx2] = 1\n",
    "\n",
    "            else:\n",
    "                y2_pred_threshold[idx1, idx2] = 0\n",
    "    \n",
    "    return y2_pred_threshold\n",
    "\n",
    "def predict_accuracy(y_test, y2_pred_threshold, butterflies):\n",
    "    \n",
    "    acc_vlinder_1 = accuracy_score_butterfly1 = accuracy_score(y_test[butterflies[0], 0], y2_pred_threshold[butterflies[0], 0])\n",
    "    acc_vlinder_2 = accuracy_score_butterfly2 = accuracy_score(y_test[butterflies[1], 1], y2_pred_threshold[butterflies[1], 1])\n",
    "    acc_vlinder_3 = accuracy_score_butterfly3 = accuracy_score(y_test[butterflies[2], 2], y2_pred_threshold[butterflies[2], 2])\n",
    "    acc_vlinder_4 = accuracy_score_butterfly4 = accuracy_score(y_test[butterflies[3], 3], y2_pred_threshold[butterflies[3], 3])\n",
    "    acc_vlinder_5 = accuracy_score_butterfly5 = accuracy_score(y_test[butterflies[4], 4], y2_pred_threshold[butterflies[4], 4])\n",
    "    \n",
    "    return acc_vlinder_1, acc_vlinder_2, acc_vlinder_3, acc_vlinder_4, acc_vlinder_5\n",
    "\n",
    "\n",
    "def scatter_plot(x, y, x_label, y_label, title, threshold):\n",
    "    plt.scatter(x, y)\n",
    "    plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def boxplot(my_dict,x_label, y_label, title,threshold):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(my_dict.values())\n",
    "    ax.set_xticklabels(my_dict.keys())\n",
    "    plt.axhline(y=threshold, color='r', linestyle='-')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "def analyse(X_train, X_test, y_train, y_test, model, thresh):\n",
    "    y_train, y_test = pd.get_dummies(y_train), pd.get_dummies(y_test)\n",
    "    y_test = np.asarray(y_test).astype('float32').reshape((-1,5))\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    i_list_vlinder_1, i_list_vlinder_2, i_list_vlinder_3, i_list_vlinder_4, i_list_vlinder_5 = get_index_of_observations(y_test)\n",
    "    \n",
    "    butterflies = []\n",
    "    butterflies.append(i_list_vlinder_1)\n",
    "    butterflies.append(i_list_vlinder_2)\n",
    "    butterflies.append(i_list_vlinder_3)\n",
    "    butterflies.append(i_list_vlinder_4)\n",
    "    butterflies.append(i_list_vlinder_5)\n",
    "\n",
    "    y_pred_threshold = threshold(thresh, y_pred)\n",
    "\n",
    "    accuracy_score_butterfly1, accuracy_score_butterfly2, accuracy_score_butterfly3, accuracy_score_butterfly4, accuracy_score_butterfly5 = predict_accuracy(y_test, y_pred_threshold, butterflies)\n",
    "\n",
    "    y_pred_threshold = threshold(thresh, y_pred)\n",
    "\n",
    "    \n",
    "    #scatterplots\n",
    "    scatter_plot(i_list_vlinder_1, y_pred[butterflies[0],0], \"\", \"prediction value\", \"scatterplot butterfly 1 acc: \" + str(accuracy_score_butterfly1), thresh)\n",
    "    scatter_plot(i_list_vlinder_2, y_pred[butterflies[1],1], \"\", \"prediction value\", \"scatterplot butterfly 2 acc: \" + str(accuracy_score_butterfly2), thresh)\n",
    "    scatter_plot(i_list_vlinder_3, y_pred[butterflies[2],2], \"\", \"prediction value\", \"scatterplot butterfly 3 acc: \" + str(accuracy_score_butterfly3), thresh)\n",
    "    scatter_plot(i_list_vlinder_4, y_pred[butterflies[3],3], \"\", \"prediction value\", \"scatterplot butterfly 4 acc: \" + str(accuracy_score_butterfly4), thresh)\n",
    "    scatter_plot(i_list_vlinder_5, y_pred[butterflies[4],4], \"\", \"prediction value\", \"scatterplot butterfly 5 acc: \" + str(accuracy_score_butterfly5), thresh)\n",
    "    \n",
    "    #boxplot\n",
    "    my_dict = {'v1': y_pred[i_list_vlinder_1,0], \n",
    "           'v2': y_pred[i_list_vlinder_2,1], \n",
    "           'v3': y_pred[i_list_vlinder_3,2], \n",
    "           'v4': y_pred[i_list_vlinder_4,3], \n",
    "           'v5': y_pred[i_list_vlinder_5,4]}\n",
    "    \n",
    "    boxplot(my_dict,\"\", \"prediction value\", \"boxplot of all butterflies\", thresh)\n",
    "    \n",
    "    #classification report\n",
    "    print(classification_report(y_test, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024983,
     "end_time": "2020-11-18T12:30:36.410376",
     "exception": false,
     "start_time": "2020-11-18T12:30:36.385393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Final data preperations\n",
    "## From laurens we recieved the following points of feedback:\n",
    "1. Make use of different loss and accuracy functions\n",
    "1. Make a selection of two types of butterflies\n",
    "1. Less layers to train with\n",
    "1. Keep decreasing the number of neurons as the layers continue\n",
    "1. Check the output\n",
    "1. We dont have a classification problem but a multilabel-problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025001,
     "end_time": "2020-11-18T12:30:36.460789",
     "exception": false,
     "start_time": "2020-11-18T12:30:36.435788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Accuracy:\n",
    "https://keras.io/api/metrics/accuracy_metrics/#sparsetopkcategoricalaccuracy-class\n",
    "<br>\n",
    "Loss:\n",
    "https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(repo_path / 'data/prepared/data_merged.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032758,
     "end_time": "2020-11-18T12:30:36.524562",
     "exception": false,
     "start_time": "2020-11-18T12:30:36.491804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating a selection of a few butterflies with different patterns (POC)\n",
    "\n",
    "The selection was made by Vincent Kalkman, which is a known biologist from Naturalis and gave us a list of butterflies that we can use:\n",
    "- Parnassius apollo \n",
    "- Zerynthia rumina\n",
    "- Agriades optilete\n",
    "- Melanargia galathea\n",
    "- Boloria titania \n",
    "\n",
    "By using a subset with only 5 butterflies, it is much easier to make a proof of concept because now, it will take less time to train the model, and also these butterflies are all located at different parts of Europe, so that will make the improvements made to the model easier to spot.\n",
    "\n",
    "Below we will:\n",
    "\n",
    "1. Make a DataFrame with only the selected butterflies (only for POC).\n",
    "1. Count the amount of each butterfly.\n",
    "1. Change the butterfly name into categorical data, to make it easier to distinguish for machines (from words to numbers)."
   ]
  },
  {
   "source": [
    "**Only run the following code when runing the proof of concept (POC)**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-18T12:30:36.634662Z",
     "iopub.status.busy": "2020-11-18T12:30:36.633961Z",
     "iopub.status.idle": "2020-11-18T12:30:38.879940Z",
     "shell.execute_reply": "2020-11-18T12:30:38.879339Z"
    },
    "papermill": {
     "duration": 2.279274,
     "end_time": "2020-11-18T12:30:38.880063",
     "exception": false,
     "start_time": "2020-11-18T12:30:36.600789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "#      To run on the full dataset, comment out the code below     #\n",
    "###################################################################\n",
    "\n",
    "butterfly_list = ['Parnassius apollo', \n",
    "                  'Zerynthia rumina', \n",
    "                  'Agriades optilete',\n",
    "                  'Melanargia galathea', \n",
    "                  'Boloria titania']\n",
    "\n",
    "# Only use selected butterflies\n",
    "df = df[df['taxon_full_name'].isin(butterfly_list)]\n",
    "\n",
    "####################################################################"
   ]
  },
  {
   "source": [
    "## Continuation of final data preperation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values\n",
    "df['taxon_full_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom species names and land climate names into categorial values\n",
    "df['taxon_full_name'], categories_taxon = create_cat_values(df['taxon_full_name'], 'taxon_full_name')\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Split the data into input (X) and output (y) values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-18T12:30:39.461355Z",
     "iopub.status.busy": "2020-11-18T12:30:39.459472Z",
     "iopub.status.idle": "2020-11-18T12:30:39.462056Z",
     "shell.execute_reply": "2020-11-18T12:30:39.462620Z"
    },
    "papermill": {
     "duration": 0.038284,
     "end_time": "2020-11-18T12:30:39.462759",
     "exception": false,
     "start_time": "2020-11-18T12:30:39.424475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Features X gets all columns except first column\n",
    "X = df.iloc[:, 1:]\n",
    "\n",
    "# Label y gets first column which is 'taxon_full_name'\n",
    "y = df.iloc[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051328,
     "end_time": "2020-11-18T12:30:40.874460",
     "exception": false,
     "start_time": "2020-11-18T12:30:40.823132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Model 1 with SpargeTopKCategoricalAccuracy (k=1)\n",
    "\n",
    "This was the first version of our Artificial Neural Network (ANN) that was used to create the SDM output. This network used `SpargeTopKCategoricalAccuracy` and `sparse_categorical_crossentropy` as metrics with `k=1`.\n",
    "\n",
    "**This was the first version and is not used any further.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-18T12:30:41.232095Z",
     "iopub.status.busy": "2020-11-18T12:30:41.231410Z",
     "iopub.status.idle": "2020-11-18T12:32:09.183719Z",
     "shell.execute_reply": "2020-11-18T12:32:09.183036Z"
    },
    "papermill": {
     "duration": 87.997057,
     "end_time": "2020-11-18T12:32:09.183863",
     "exception": false,
     "start_time": "2020-11-18T12:30:41.186806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the function used to train the first network\n",
    "def train_model1(X_train, X_test, y_train, y_test):\n",
    "    output_neurons = len(y_train.unique())\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_uniform'),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(56, activation='relu'),\n",
    "      tf.keras.layers.Dense(28, activation='relu'),\n",
    "      tf.keras.layers.Dense(output_neurons, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(\n",
    "        k=1, name=\"sparse_top_k_categorical_accuracy\", dtype=None\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=[acc])  \n",
    "                  \n",
    "    model_history = model1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200)\n",
    "\n",
    "    plot_sparse_accuracy('Model 1 (sparse_categorical_crossentropy 1)', model_history)\n",
    "    plot_loss('Model 1 (sparse_categorical_crossentropy 1)', model_history)\n",
    "\n",
    "    return model1\n",
    "\n",
    "# Uncomment the line below to run the function\n",
    "# model1 = train_model1(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "source": [
    "# 5. Model 2 with SpargeTopKCategoricalAccuracy (k=2)\n",
    "\n",
    "This was the second version of our ANN that was used to create the SDM output. This network used `SpargeTopKCategoricalAccuracy` and `sparse_categorical_crossentropy` as metrics with `k=2`.\n",
    "\n",
    "**This model is used to compare the perfomance between different metrics.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-18T12:32:11.799022Z",
     "iopub.status.busy": "2020-11-18T12:32:11.793705Z",
     "iopub.status.idle": "2020-11-18T12:33:39.574464Z",
     "shell.execute_reply": "2020-11-18T12:33:39.573763Z"
    },
    "papermill": {
     "duration": 88.381093,
     "end_time": "2020-11-18T12:33:39.574619",
     "exception": false,
     "start_time": "2020-11-18T12:32:11.193526",
     "status": "completed"
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the function used to train the second network\n",
    "def train_model2(X_train, X_test, y_train, y_test):\n",
    "    output_neurons = len(y_train.unique())\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(256, activation='relu', kernel_initializer='he_uniform'),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(56, activation='relu'),\n",
    "      tf.keras.layers.Dense(28, activation='relu'),\n",
    "      tf.keras.layers.Dense(output_neurons, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(\n",
    "        k=2, name=\"sparse_top_k_categorical_accuracy\", dtype=None\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=[acc])  \n",
    "\n",
    "    model_history = model.fit(X_train, \n",
    "                              y_train, \n",
    "                              validation_data=(X_test, y_test), \n",
    "                              epochs=200)\n",
    "\n",
    "    plot_sparse_accuracy('Model 2 (sparse_categorical_crossentropy 2)', model_history)\n",
    "    plot_loss('Model 2 (sparse_categorical_crossentropy 2)', model_history)\n",
    "\n",
    "    return model\n",
    "\n",
    "model2 = train_model2(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "source": [
    "# 6. Model 3 with Binary-Crossentropy (final base model, without optimalisations)\n",
    "Below you can see how we trained the final base model. This model has 5 Dense layers, wich 256, 128, 56, and 28 neurons respectively. The first 4 use relu activation, and the last uses sigmoid activation. \n",
    "\n",
    "We are also using the optimizer Adam with a learning rate of 0.001, the loss function binary-crossentropy, and with the regular accuracy metric.\n",
    "\n",
    "This function can also be enabled to use oversampling, normalizing the amount of each butterfly, to make them equal. However, this is not used right now but will be used later on in the notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# This is the function used to train the final base model\n",
    "def train_model3(X_train, X_test, y_train, y_test, oversample=False, verbose=1):\n",
    "  \"\"\"\n",
    "  X_train    = data that the model is trained with\n",
    "  X_test     = data that the model is tested with\n",
    "  y_train    = the species you are trying to predict during training\n",
    "  y_test     = the species you are trying to predict during testing\n",
    "  oversample = enables oversampling of the training data\n",
    "  verbose    = 1: prints training info. 0: does not print output training info\n",
    "  \"\"\"\n",
    "  y_train, y_test = pd.get_dummies(y_train), pd.get_dummies(y_test)\n",
    "  output_neurons = len(y_train.columns)\n",
    "\n",
    "  if oversample:\n",
    "    oversampler = RandomOverSampler()\n",
    "    X_train, y_train = oversampler.fit_resample(X_train.to_numpy(), y_train.to_numpy())\n",
    "    output_neurons = len(y_train[0])\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, \n",
    "                      activation='relu',\n",
    "                      kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(56, activation='relu'),\n",
    "    tf.keras.layers.Dense(28, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_neurons, activation='sigmoid')\n",
    "  ])\n",
    "\n",
    "  opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "  model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "  \n",
    "  model_history = model.fit(X_train, \n",
    "                            y_train, \n",
    "                            validation_data=(X_test, y_test), \n",
    "                            epochs=200, \n",
    "                            batch_size=100,\n",
    "                            verbose=verbose)\n",
    "\n",
    "  if verbose == 1:\n",
    "    plot_accuracy('Model 3 (binary_crossentropy)', model_history)\n",
    "    plot_loss('Model 3 (binary_crossentropy)', model_history)\n",
    "\n",
    "  return model\n",
    "\n",
    "model3 = train_model3(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "source": [
    "# 7. Visualize the predictions with the final base model\n",
    "Now it is time to start making visualisations, and see wheter the SDM is accurate.\n",
    "\n",
    "Below we will:\n",
    "\n",
    "1. Import the European environment data to make predicitions on.\n",
    "1. Change the land climate name into categorical data, to make it easier to distinguish for machines (from words to numbers).\n",
    "1. Remove allw water bodies, because butterflies can't live on water\n",
    "1. Normalize the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full Europe environment dataset\n",
    "df_env = pd.read_csv(repo_path / 'data/prepared/data_env_fixed_prepared.csv', index_col=0)\n",
    "\n",
    "# Categorize Land Climate data\n",
    "df_env['land_climate'], categories_land = create_cat_values(df_env['land_climate'], 'land_climate')\n",
    "\n",
    "# Remove all water bodies from dataset\n",
    "df_env_no_water = df_env[df_env['land_climate'] != 'Water Bodies']\n",
    "df_env_no_water.dropna(inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "column_to_norm = ['elevation', 'WorldClim_prec_m06', 'WorldClim_srad_m06',\n",
    "       'WorldClim_tavg_m06', 'WorldClim_tmax_m06', 'WorldClim_tmin_m06',\n",
    "       'WorldClim_vapr_m06', 'WorldClim_wind_m06', 'BioClim_01', 'BioClim_02',\n",
    "       'BioClim_03', 'BioClim_04', 'BioClim_05', 'BioClim_06', 'BioClim_07',\n",
    "       'BioClim_08', 'BioClim_09', 'BioClim_10', 'BioClim_11', 'BioClim_12',\n",
    "       'BioClim_13', 'BioClim_14', 'BioClim_15', 'BioClim_16', 'BioClim_17',\n",
    "       'BioClim_18', 'BioClim_19']\n",
    "df_env_no_water[column_to_norm] = df_env_no_water[column_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "df_env_no_water.head()"
   ]
  },
  {
   "source": [
    "Now, we will make predictions on how much chance a butterfly has on existing on each coordinate of the European environment dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, df_env):\n",
    "    # Predict likelyhood of butterfly being present on all locations in Europe\n",
    "    prob = model.predict(df_env)\n",
    "\n",
    "    judge_species0 = prob[:, 0]\n",
    "    judge_species1 = prob[:, 1] \n",
    "    judge_species2 = prob[:, 2]\n",
    "    judge_species3 = prob[:, 3]\n",
    "    judge_species4 = prob[:, 4]\n",
    "\n",
    "    return judge_species0, judge_species1, judge_species2, judge_species3, judge_species4\n",
    "\n",
    "# Predict likelyhood with best model (model3)\n",
    "judge_species0, judge_species1, judge_species2, judge_species3, judge_species4 = predict(model3, df_env_no_water)"
   ]
  },
  {
   "source": [
    "These two functions are able to visualise the predictions per species"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations\n",
    "def plot_positives(environment, judge, boundary):\n",
    "    positives = environment[judge >= boundary]\n",
    "\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "    ax = world.plot(color='white', edgecolor='black', figsize=(40, 40))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        positives, \n",
    "        geometry=gpd.points_from_xy(positives['lon_latlon'], \n",
    "                                    positives['lat_latlon']))\n",
    "    gdf.plot(ax=ax, markersize=15)\n",
    "\n",
    "    #Define map boundaries\n",
    "    plt.xlim(-25, 43)\n",
    "    plt.ylim(33, 72)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_probability(environment, judge, boundary):\n",
    "    positives = environment[judge >= boundary]\n",
    "    judge_positives = judge[judge >= boundary]\n",
    "\n",
    "    preds = positives[['lat_latlon', 'lon_latlon']]\n",
    "    preds['judge'] = judge_positives\n",
    "\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "    ax = world.plot(color='white', edgecolor='black', figsize=(40, 40))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        positives, \n",
    "        geometry=gpd.points_from_xy(preds['lon_latlon'], \n",
    "                                    preds['lat_latlon']))\n",
    "    gdf.plot(ax=ax, \n",
    "             column=preds['judge'], \n",
    "             legend=True, \n",
    "             markersize=15, \n",
    "             legend_kwds={'shrink': 0.5})\n",
    "\n",
    "    #Define map boundaries\n",
    "    plt.xlim(-25, 43)\n",
    "    plt.ylim(33, 72)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Returns DataFrame with locations and prediction certainty\n",
    "    return preds"
   ]
  },
  {
   "source": [
    "Below you can see the visualisations of the five butterflies, predicted using our best model (the third one using binary-crossentropy)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Species 0 (Agriades optilete)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_positives(df_env_no_water, judge_species0, .5)\n",
    "preds = plot_probability(df_env_no_water, judge_species0, .01)"
   ]
  },
  {
   "source": [
    "## Species 1 (Boloria titania)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_positives(df_env_no_water, judge_species1, .5)\n",
    "preds = plot_probability(df_env_no_water, judge_species1, .01)"
   ]
  },
  {
   "source": [
    "## Species 2 (Melanargia galathea)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_positives(df_env_no_water, judge_species2, .5)\n",
    "preds = plot_probability(df_env_no_water, judge_species2, .01)"
   ]
  },
  {
   "source": [
    "## Species 3 (Parnassius apollo)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_positives(df_env_no_water, judge_species3, .5)\n",
    "preds = plot_probability(df_env_no_water, judge_species3, .01)"
   ]
  },
  {
   "source": [
    "## Species 4 (Zerynthia rumina)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_positives(df_env_no_water, judge_species4, .5)\n",
    "preds = plot_probability(df_env_no_water, judge_species4, .01)"
   ]
  },
  {
   "source": [
    "## Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(X_train, X_test, y_train, y_test, model3, 0.5)"
   ]
  },
  {
   "source": [
    "# 8. Visualisations with SDM optimalisations\n",
    "\n",
    "This chapter will contain information on all optimalisations that were executed. None of the optimalisations have been added to the final base model, because as ICT students it was hard to decide whether a optimalisation has a positive effect\n",
    "\n",
    "Below you can see the evaluation on optimalisations of the SDM. The evaluation contains the following:\n",
    "\n",
    "- SDM output\n",
    "- Scatterplot\n",
    "- Boxplot\n",
    "- Precision, recall, and F1-score\n",
    "\n",
    "If you want indept information about the evaluation, you are referred to the final report.\n",
    "\n",
    "The following were the optimalisations tested:\n",
    "\n",
    "1. Oversampling\n",
    "1. Feature dropping\n",
    "1. Plotting predictions within X distance of nearest observation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.1. Oversampling\n",
    "\n",
    "The visualisations above all look great, but now, let us check wheter using ovesampling will increase the model's perfomance.\n",
    "\n",
    "So we will train our final base model (model 3) with oversampling enabled, and predict how much chance a butterfly has on existing on each coordinate of the European environment dataset again.\n",
    "\n",
    "Oversampling is a technique that multiplace the categories with less data values and increases its data size. For this project the approach taken with oversampling was multiplying all the values to the same amount as species with most observations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_oversample = train_model3(X_train, X_test, y_train, y_test, oversample=True, verbose=0)\n",
    "judge_species0_oversample, judge_species1_oversample, judge_species2_oversample, judge_species3_oversample, judge_species4_oversample = predict(model3_oversample, df_env_no_water)"
   ]
  },
  {
   "source": [
    "### Normal visualisation (final base model)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species0, .5)"
   ]
  },
  {
   "source": [
    "### Visualisation with oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species0_oversample, .5)"
   ]
  },
  {
   "source": [
    "As you can see oversampling improves the model's certainty percentage at places it wasn't present. This changes the output of the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(X_train, X_test, y_train, y_test, model3_oversample, 0.5)"
   ]
  },
  {
   "source": [
    "## 8.2. Feature dropping (coordinates)\n",
    "Now let us experiment with dropping the coordinates, to see how the output of the model changes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the coordinates from the butterflies and training the third model again\n",
    "X_without_coordinates = X.drop(['obs_latitude', 'obs_longitude'], axis=1)\n",
    "X_noco_train, X_noco_test, y_noco_train, y_noco_test = train_test_split(X_without_coordinates, y, test_size=0.33, random_state=42)\n",
    "model3_without_coordinates = train_model3(X_noco_train, X_noco_test, y_noco_train, y_noco_test, verbose=0)\n",
    "\n",
    "# Removing the coordinates from the environment dataset\n",
    "df_env_no_water_coordinates = df_env_no_water.drop(['lat_latlon', 'lon_latlon'], axis=1)\n",
    "\n",
    "# Making new predictions\n",
    "judge_species0_without_coordinates, judge_species1_without_coordinates, judge_species2_without_coordinates, judge_species3_without_coordinates, judge_species4_without_coordinates = predict(model3_without_coordinates, df_env_no_water_coordinates)"
   ]
  },
  {
   "source": [
    "### Normal visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species4, .01)"
   ]
  },
  {
   "source": [
    "### Visualisation with feature dropping (coordinates)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species4_without_coordinates, .01)"
   ]
  },
  {
   "source": [
    "We can see that the results here have less impact on the certainty percentage, but more on the distribution of a butterfly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse(X_noco_train, X_noco_test, y_noco_train, y_noco_test, model3_without_coordinates, 0.5)"
   ]
  },
  {
   "source": [
    "## 8.3 Plotting predictions within X distance of nearest observation\n",
    "\n",
    "Vincent came with a request to not plot predictions that were further than distance X from the nearest observation. For the prototype we created a function which is able to draw the maximum and minimum latitude and longitude values, which then creates a rectengle with the species living habitat. It is important to know the function is not at its final stages, and will need to be researched futher, as having multiple cluster of the same species will bring problems.\n",
    "\n",
    "In the POC the distance of 1 latitude and longitude point was used, which translates to about 110km."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Max/Min Lat/Lon values for a species\n",
    "def get_maxcoordinates(df, species, lat_lon_points):\n",
    "    df_spec = df[df['taxon_full_name'] == species]\n",
    "    maxminval = []\n",
    "    latmax = df_spec['obs_latitude'].max() + lat_lon_points\n",
    "    latmin = df_spec['obs_latitude'].min() - lat_lon_points\n",
    "    lonmax = df_spec['obs_longitude'].max() + lat_lon_points\n",
    "    lonmin = df_spec['obs_longitude'].min() - lat_lon_points\n",
    "    maxminval.append(latmax)\n",
    "    maxminval.append(latmin)\n",
    "    maxminval.append(lonmax)\n",
    "    maxminval.append(lonmin)\n",
    "    return maxminval\n",
    "\n",
    "# This is the same function as plot_probability, now filtering out the values between the maximum and minimum latitiude/longitude\n",
    "def plot_probability_distancefurther_x(environment, judge, boundary, maxmin):\n",
    "    positives = environment[judge >= boundary]\n",
    "    judge_positives = judge[judge >= boundary]\n",
    "\n",
    "    preds = positives[['lat_latlon', 'lon_latlon']]\n",
    "    preds['judge'] = judge_positives\n",
    "    preds = preds[preds[\"lat_latlon\"] < maxmin[0]]\n",
    "    preds = preds[preds[\"lat_latlon\"] > maxmin[1]]\n",
    "    preds = preds[preds[\"lon_latlon\"] < maxmin[2]]\n",
    "    preds = preds[preds[\"lon_latlon\"] > maxmin[3]]\n",
    "    preds.reset_index()\n",
    "\n",
    "    positives = positives[positives[\"lat_latlon\"] < maxmin[0]]\n",
    "    positives = positives[positives[\"lat_latlon\"] > maxmin[1]]\n",
    "    positives = positives[positives[\"lon_latlon\"] < maxmin[2]]\n",
    "    positives = positives[positives[\"lon_latlon\"] > maxmin[3]]\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "    ax = world.plot(color='white', edgecolor='black', figsize=(40, 40))\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "    positives,\n",
    "    geometry=gpd.points_from_xy(preds['lon_latlon'],\n",
    "    preds['lat_latlon']))\n",
    "    gdf.plot(ax=ax,\n",
    "    column=preds['judge'],\n",
    "    legend=True,\n",
    "    markersize=15,\n",
    "    legend_kwds={'shrink': 0.5})\n",
    "\n",
    "    #Define map boundaries\n",
    "    plt.xlim(-25, 43)\n",
    "    plt.ylim(33, 72)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Returns DataFrame with locations and prediction certainty\n",
    "    return preds"
   ]
  },
  {
   "source": [
    "### Normal visualisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species4, .01)"
   ]
  },
  {
   "source": [
    "### Visualisation with predictions within 1 distance of nearest observation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxminval = get_maxcoordinates(df, 4, 1)\n",
    "preds = plot_probability_distancefurther_x(df_env_no_water, judge_species4, .5, maxminval)"
   ]
  },
  {
   "source": [
    "This function works as expected"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## All visualisations\n",
    "### Visualisation base model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species4, .01)"
   ]
  },
  {
   "source": [
    "### Visualisation with oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species4_oversample, .5)"
   ]
  },
  {
   "source": [
    "### Visualisation with feature dropping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plot_probability(df_env_no_water, judge_species4_without_coordinates, .01)"
   ]
  },
  {
   "source": [
    "### Visualisation with predictions within 1 distance of nearest observation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxminval = get_maxcoordinates(df, 4, 1)\n",
    "preds = plot_probability_distancefurther_x(df_env_no_water, judge_species4, .5, maxminval)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "papermill": {
   "duration": 328.20954,
   "end_time": "2020-11-18T12:35:54.231346",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-18T12:30:26.021806",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}